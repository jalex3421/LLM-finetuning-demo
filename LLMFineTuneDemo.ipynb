{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "94e2c6ac3596440a9669c1909c2f7188": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c8b865e82416457cb44a804b3e1d9d84",
              "IPY_MODEL_fc624a00fa1d4651b0f1484b2a97949d",
              "IPY_MODEL_ca24225e67b34d70bdedb5151839c7bc"
            ],
            "layout": "IPY_MODEL_bf7d1e88afb743c48763723617b83d34"
          }
        },
        "c8b865e82416457cb44a804b3e1d9d84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2b283c12b53419ca78515bdc488c265",
            "placeholder": "​",
            "style": "IPY_MODEL_acc4dc146ede482aaf7e8dfc46f07e7b",
            "value": "Unsloth: Tokenizing [&quot;text&quot;] (num_proc=6): 100%"
          }
        },
        "fc624a00fa1d4651b0f1484b2a97949d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5dbbfc655a6d48338d4afca1795a1099",
            "max": 13,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d4b43bfe12a148df959b8f386bf775bf",
            "value": 13
          }
        },
        "ca24225e67b34d70bdedb5151839c7bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54be2190f0d948c88369f112a3eca12b",
            "placeholder": "​",
            "style": "IPY_MODEL_b16f5227923e4904a065757184d57f03",
            "value": " 13/13 [00:11&lt;00:00,  1.85 examples/s]"
          }
        },
        "bf7d1e88afb743c48763723617b83d34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2b283c12b53419ca78515bdc488c265": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acc4dc146ede482aaf7e8dfc46f07e7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5dbbfc655a6d48338d4afca1795a1099": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4b43bfe12a148df959b8f386bf775bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "54be2190f0d948c88369f112a3eca12b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b16f5227923e4904a065757184d57f03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LLM fine-tune code\n",
        "\n",
        "Author: Alejandro Meza\n",
        "\n",
        "This notebook demonstrates the full process:\n",
        "- **1. Loading a base model**\n",
        "- **2. Applying LoRA adapters for efficient fine-tuning**\n",
        "- **3. Formatting a custom dataset**\n",
        "- **4. Training the model with SFTTrainer**\n",
        "- **5. Running inference to test the fine-tuned LLM**\n",
        "\n",
        "For this notebook, we are gonna use **Unsloth**. Unsloth is an open-source framework / platform for fine-tuning, training, and reinforcement learning (RL) of large language models (LLMs).  Its goal is to make creating custom LLMs faster, cheaper (in terms of GPU / memory usage), and more accessible to users.\n",
        "\n",
        "**Acknowledgment**: Special thanks to the YouTube channel Tech with Tim for the video \"[EASIEST Way to Fine-Tune a LLM and Use It With Ollama]\", which greatly inspired the creation of this notebook. The content of the video served as a great reference."
      ],
      "metadata": {
        "id": "t8wMvYfiUc0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## JSON -input data- definition\n",
        "\n",
        "Define a sample dataset to fine-tune our LLM. This time, the sample dataset is gonna be a **JJK** dataset!\n",
        "\n"
      ],
      "metadata": {
        "id": "nlnVC2duUfJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "json_demo = [\n",
        "  {\n",
        "    \"instruction\": \"Summarize the following Jujutsu Kaisen scene.\",\n",
        "    \"input\": \"During the Shibuya Incident, Satoru Gojo single-handedly fights off numerous cursed spirits and special grade sorcerers inside a crowded station.\",\n",
        "    \"output\": \"Gojo dominates multiple powerful enemies in Shibuya Station during the Shibuya Incident, showcasing his overwhelming strength.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"Translate this Jujutsu Kaisen quote to English.\",\n",
        "    \"input\": \"俺は最強だ。\",\n",
        "    \"output\": \"I am the strongest.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"List three main abilities of Satoru Gojo.\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"1. Limitless (infinite manipulation of space)\\n2. Six Eyes (enhanced perception and cursed energy control)\\n3. Hollow Purple (destructive technique combining Blue and Red)\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"Write a short dialogue between Yuji Itadori and Megumi Fushiguro about training.\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"Yuji: \\\"Megumi, let's spar again! I need to get faster.\\\"\\nMegumi: \\\"You just fought Sukuna inside your own body. Maybe rest first.\\\"\\nYuji: \\\"Rest is for the weak!\\\"\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"Classify the following character as sorcerer, curse, or non-sorcerer.\",\n",
        "    \"input\": \"Mahito\",\n",
        "    \"output\": \"curse\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"Describe the Black Flash technique in one sentence.\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"Black Flash is a distorted space attack where cursed energy strikes within 0.000001 seconds of a physical hit, amplifying its power.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"Explain the relationship between Yuji Itadori and Sukuna in two sentences.\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"Yuji Itadori became Sukuna’s vessel after consuming one of his cursed fingers. Sukuna can manifest within Yuji’s body, creating a constant struggle for control.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"Given the following characters, identify which one possesses the Ten Shadows Technique.\",\n",
        "    \"input\": \"Choices: [Megumi Fushiguro, Nobara Kugisaki, Kento Nanami]\",\n",
        "    \"output\": \"Megumi Fushiguro\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"Write a motivational quote Gojo might say to inspire his students.\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"\\\"Strength isn't just about cursed energy—it's about believing that you can always surpass your limits.\\\"\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"Predict what strategy Megumi would use if facing a stronger opponent alone.\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"Megumi would summon multiple shikigami to create openings and strategically retreat while analyzing the opponent’s weaknesses.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"Translate this character name into kanji if possible.\",\n",
        "    \"input\": \"Yuji Itadori\",\n",
        "    \"output\": \"虎杖悠仁\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"Provide a one-sentence description of Nobara Kugisaki’s combat style.\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"Nobara fights with a combination of cursed nails, a hammer, and Straw Doll Technique to attack enemies from a distance or inflict resonant damage.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"Summarize the importance of cursed fingers in the story.\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"Sukuna’s cursed fingers are powerful remnants that contain fragments of his soul, driving the plot as both sorcerers and curses seek them to gain power or prevent disaster.\"\n",
        "  }\n",
        "]"
      ],
      "metadata": {
        "id": "7wW_qiUIUi4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start the code!"
      ],
      "metadata": {
        "id": "czKb87OpU6Ff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install the main dependencies"
      ],
      "metadata": {
        "id": "LxA_pvDpnZ0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "SVueBJuFVFpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install unsloth trl peft accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "2CL1-QDNU8Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check running device"
      ],
      "metadata": {
        "id": "qieNDlz_ne6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#device check\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTPA0oh9VQGt",
        "outputId": "3a4f1b87-2462-4796-ffb1-721a26ddfa26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import model"
      ],
      "metadata": {
        "id": "-7SUjWxEniKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Name of the base model to fine-tune or run inference with  (quantized model of 4 bits)\n",
        "# You can select other models from here: https://docs.unsloth.ai/get-started/all-our-models\n",
        "model_name = \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\"\n",
        "\n",
        "# Maximum sequence length (context window) the model will handle during training/inference.\n",
        "# Increase if your dataset contains longer prompts/responses (must be supported by the model).\n",
        "max_seq_length = 2048\n",
        "\n",
        "# Data type for model weights/activations.\n",
        "# None = let Unsloth/transformers automatically choose the most efficient dtype (fp16/bf16).\n",
        "dtype = None\n",
        "\n",
        "# Load both the model and its tokenizer.\n",
        "# - load_in_4bit=True loads a 4-bit quantized checkpoint to drastically reduce VRAM usage.\n",
        "# - max_seq_length sets the context window used by the model and tokenizer.\n",
        "# - dtype selects the precision (auto if None).\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVhXfXB4VX6l",
        "outputId": "857801e9-5cc2-4108-91ce-68de902874ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.9.11: Fast Gemma3 patching. Transformers: 4.56.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Using float16 precision for gemma3 won't work! Using float32.\n",
            "Unsloth: Gemma3 does not support SDPA - switching to fast eager.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Format our data"
      ],
      "metadata": {
        "id": "QT5LuS3aoevX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define a function for format our text\n",
        "from datasets import Dataset\n",
        "import json\n",
        "\n",
        "def format_prompt(example):\n",
        "    return (\n",
        "        f\"### Instruction:\\n{example['instruction']}\\n\\n\"\n",
        "        f\"### Input:\\n{example['input']}\\n\\n\"\n",
        "        f\"### Response:\\n{example['output']}<|endoftext|>\"\n",
        "    )\n",
        "\n",
        "formatted_data = [format_prompt(item) for item in json_demo]\n",
        "dataset = Dataset.from_dict({\"text\": formatted_data})\n",
        "print(dataset[0][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2cGDlCUV_3D",
        "outputId": "3c648d35-637a-4e6f-9fa9-2022c529501d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Instruction:\n",
            "Summarize the following Jujutsu Kaisen scene.\n",
            "\n",
            "### Input:\n",
            "During the Shibuya Incident, Satoru Gojo single-handedly fights off numerous cursed spirits and special grade sorcerers inside a crowded station.\n",
            "\n",
            "### Response:\n",
            "Gojo dominates multiple powerful enemies in Shibuya Station during the Shibuya Incident, showcasing his overwhelming strength.<|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "QcnxbvQxok3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To fine-tune a large language model efficiently, we use LoRA (Low-Rank Adaptation) adapters.\n",
        "# LoRA allows us to train only a small subset of weights while keeping the majority of the model frozen,\n",
        "# which drastically reduces memory usage and speeds up training.\n",
        "# LoRA paper: https://arxiv.org/abs/2106.09685\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "\n",
        "    # LoRA rank: determines the dimensionality of the low-rank matrices.\n",
        "    # Higher rank = closer to full model performance but uses more memory.\n",
        "    r=64,\n",
        "\n",
        "    # Target modules in the model where LoRA adapters will be applied.\n",
        "    # Common modules for transformers: query, key, value, output projections, and feed-forward projections.\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "\n",
        "    # LoRA scaling factor: often set to 2x rank. Scales the low-rank update during training.\n",
        "    lora_alpha=132,\n",
        "\n",
        "    # Dropout applied to LoRA weights during training.\n",
        "    lora_dropout=0,\n",
        "\n",
        "    # Bias setting for LoRA layers. \"none\" is recommended for efficiency.\n",
        "    bias=\"none\",\n",
        "\n",
        "    # Use gradient checkpointing to save memory.\n",
        "    # \"unsloth\" uses the library’s optimized version for faster backpropagation.\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "\n",
        "    # Seed for reproducibility\n",
        "    random_state=3407,\n",
        "\n",
        "    # Whether to use Rank-Stabilized LoRA (optional advanced method)\n",
        "    use_rslora=False,\n",
        "\n",
        "    # LoftQ configuration (advanced quantization for LoRA, can be left None)\n",
        "    loftq_config=None,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuGmkeiiWIXV",
        "outputId": "ecc6d084-c770-4266-8176-fdde364784f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Making `model.base_model.model.model` require gradients\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# ----------------------------\n",
        "# Define fine-tuning hyperparameters\n",
        "# ----------------------------\n",
        "NUM_EPOCHS = 5         # Number of full passes over the dataset\n",
        "LEARNING_RATE = 2e-4   # Learning rate for optimizer\n",
        "\n",
        "# ----------------------------\n",
        "# Set up the SFTTrainer (Supervised Fine-Tuning)\n",
        "# ----------------------------\n",
        "trainer = SFTTrainer(\n",
        "    model=model,                  # The LoRA-adapted model\n",
        "    tokenizer=tokenizer,          # Tokenizer corresponding to the model\n",
        "    train_dataset=dataset,        # Dataset to fine-tune on (already formatted)\n",
        "    dataset_text_field=\"text\",    # Field in dataset containing training text\n",
        "    max_seq_length=max_seq_length,# Max sequence length for tokenization\n",
        "    dataset_num_proc=2,           # Number of processes for dataset preprocessing\n",
        "\n",
        "    args=TrainingArguments(\n",
        "        # ------------------------\n",
        "        # Batch & gradient settings\n",
        "        # ------------------------\n",
        "        per_device_train_batch_size=2,    # Number of samples per GPU per step\n",
        "        gradient_accumulation_steps=4,   # Accumulate gradients to simulate larger batch size\n",
        "                                          # Effective batch size = 2 * 4 = 8\n",
        "\n",
        "        # ------------------------\n",
        "        # Training schedule\n",
        "        # ------------------------\n",
        "        warmup_steps=10,                 # Linear warmup to stabilize initial training\n",
        "        num_train_epochs=NUM_EPOCHS,     # Number of epochs\n",
        "        learning_rate=LEARNING_RATE,     # Optimizer learning rate\n",
        "\n",
        "        # ------------------------\n",
        "        # Precision settings\n",
        "        # ------------------------\n",
        "        fp16=not torch.cuda.is_bf16_supported(),  # Use FP16 if BF16 not available\n",
        "        bf16=torch.cuda.is_bf16_supported(),     # Use BF16 if supported (faster & memory efficient)\n",
        "\n",
        "        # ------------------------\n",
        "        # Logging & optimization\n",
        "        # ------------------------\n",
        "        logging_steps=25,                # Log training metrics every N steps\n",
        "        optim=\"adamw_8bit\",             # Memory-efficient 8-bit AdamW optimizer\n",
        "        weight_decay=0.01,               # L2 regularization to prevent overfitting\n",
        "        lr_scheduler_type=\"linear\",      # Linear LR decay\n",
        "\n",
        "        # ------------------------\n",
        "        # Reproducibility & saving\n",
        "        # ------------------------\n",
        "        seed=3407,                       # Random seed for reproducibility\n",
        "        output_dir=\"outputs\",            # Directory to save checkpoints\n",
        "        save_strategy=\"epoch\",           # Save model every epoch\n",
        "        save_total_limit=2,              # Keep only the 2 most recent checkpoints\n",
        "\n",
        "        # ------------------------\n",
        "        # Performance tweaks\n",
        "        # ------------------------\n",
        "        dataloader_pin_memory=False,     # Avoid extra pinned memory (optional)\n",
        "        report_to=\"none\",                # Disable external logging (e.g., Weights & Biases)\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "94e2c6ac3596440a9669c1909c2f7188",
            "c8b865e82416457cb44a804b3e1d9d84",
            "fc624a00fa1d4651b0f1484b2a97949d",
            "ca24225e67b34d70bdedb5151839c7bc",
            "bf7d1e88afb743c48763723617b83d34",
            "c2b283c12b53419ca78515bdc488c265",
            "acc4dc146ede482aaf7e8dfc46f07e7b",
            "5dbbfc655a6d48338d4afca1795a1099",
            "d4b43bfe12a148df959b8f386bf775bf",
            "54be2190f0d948c88369f112a3eca12b",
            "b16f5227923e4904a065757184d57f03"
          ]
        },
        "id": "U7sAof95WaRk",
        "outputId": "1f231cc3-839b-49f6-f15c-bf118e1fccba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Switching to float32 training since model cannot work with float16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/13 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "94e2c6ac3596440a9669c1909c2f7188"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================== Train the model ====================================\n",
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "Lc__AZsDW22J",
        "outputId": "d05024a1-01c5-4a29-e381-cbdabe5cdfdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 13 | Num Epochs = 5 | Total steps = 10\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 52,183,040 of 1,052,068,992 (4.96% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 01:07, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Enable optimized inference\n",
        "# ----------------------------\n",
        "# FastLanguageModel.for_inference enables Unsloth's native optimizations\n",
        "# which can make generation up to 2x faster on GPU without changing model weights.\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# ----------------------------\n",
        "# Define a test prompt\n",
        "# ----------------------------\n",
        "# We provide a chat-style input to the fine-tuned model.\n",
        "# Here, we ask a Jujutsu Kaisen–related question to see if the model learned the dataset.\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": (\n",
        "            \"Explain the relationship between Yuji Itadori and Sukuna in Jujutsu Kaisen?\"\n",
        "        )\n",
        "    }\n",
        "]\n",
        "\n",
        "# ----------------------------\n",
        "# Tokenize the prompt\n",
        "# ----------------------------\n",
        "# apply_chat_template converts chat messages into a single input string for the model.\n",
        "# - tokenize=True: converts text to input IDs\n",
        "# - add_generation_prompt=True: appends special tokens that signal model to generate a response\n",
        "# - return_tensors=\"pt\": returns PyTorch tensors\n",
        "# Finally, we move tensors to GPU for fast generation.\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "# ----------------------------\n",
        "# Generate response from the fine-tuned model\n",
        "# ----------------------------\n",
        "# Parameters:\n",
        "# - max_new_tokens: maximum tokens to generate\n",
        "# - use_cache: speed optimization\n",
        "# - temperature: controls randomness (0.7 = moderately creative)\n",
        "# - do_sample=True: enables sampling instead of greedy decoding\n",
        "# - top_p=0.9: nucleus sampling to improve output quality\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs,\n",
        "    max_new_tokens=256,\n",
        "    use_cache=True,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        "    top_p=0.9,\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# Decode and print the response\n",
        "# ----------------------------\n",
        "# batch_decode converts token IDs back into readable text.\n",
        "response = tokenizer.batch_decode(outputs)[0]\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bBn7tBzW599",
        "outputId": "be74f503-ba50-4cf2-bdd0-e5ff60c1668c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "Explain the relationship between Yuji Itadori and Sukuna in Jujutsu Kaisen?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Okay, let’s break down the incredibly complex and central relationship between Yuji Itadori and Sukuna in Jujutsu Kaisen. It’s arguably the most significant and dangerous aspect of his cursed technique. Here’s a layered explanation:\n",
            "\n",
            "**1. Sukuna’s Manifestation in Yuji’s Body:**\n",
            "\n",
            "* **The Core Concept:** Sukuna isn’t just cursed energy within Yuji. Instead, Sukuna *manifests within* his own body, particularly in his cursed technique, cursed energy coils. Yuji constantly manipulates his cursed energy to create and control Sukuna’s influence.\n",
            "* **Yuji’s Cursed Energy Coils:** Yuji creates multiple cursed energy coils around himself – each one representing a different Sukuna fragment. These coils aren’t just for combat; they’re intensely cursed energy that can be manipulated for various effects.\n",
            "\n",
            "\n",
            "**2. Sukuna’s Influence & Yuji’s Weakness:**\n",
            "\n",
            "* **Eternal Remnant:** Sukuna doesn't just *exist* within Yuji. Instead, Sukuna’s influence is constantly pulling fragments of his consciousness into his own body. This creates multiple curses within him simultaneously.\n",
            "* **Yuji’s Burden:** Y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the model"
      ],
      "metadata": {
        "id": "ZpEsGka5_K7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade \"protobuf<3.21\""
      ],
      "metadata": {
        "id": "K5pMFhzA_Mwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the weights of the model\n",
        "model.save_pretrained_gguf(\"gguf_model\", tokenizer, quantization_method=\"q4_k_m\")"
      ],
      "metadata": {
        "id": "8-3jHcChcH_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "gguf_files = [f for f in os.listdir(\"gguf_model\") if f.endswith(\".gguf\")]\n",
        "if gguf_files:\n",
        "    gguf_file = os.path.join(\"gguf_model\", gguf_files[0])\n",
        "    print(f\"Downloading: {gguf_file}\")\n",
        "    files.download(gguf_file)"
      ],
      "metadata": {
        "id": "kqkWWyr3cNjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "1. [EASIEST Way to Fine-Tune a LLM and Use It With Ollama\n",
        "]  https://www.youtube.com/watch?v=pTaSDVz0gok&t=1062s"
      ],
      "metadata": {
        "id": "mD6NdiOPd3EE"
      }
    }
  ]
}