# ğŸ¦¥ğŸ’¡ LLM Fine-Tuning with Unsloth

Author: Alejandro Meza Tudela

Welcome to my notebook on **fine-tuning Large Language Models (LLMs)** using [Unsloth](https://github.com/unslothai/unsloth) 
This guide walks you through the **end-to-end process** of customizing an LLM for your own dataset â€” all while keeping things **efficient, affordable, and fun**.  

---

## ğŸ“š What Youâ€™ll Learn
This notebook demonstrates:

1. ğŸ“‚ **Define a custom dataset**  
2. ğŸ—ï¸ **Load a base model**  
3. ğŸ›ï¸ **Apply LoRA adapters** for efficient fine-tuning  
4. ğŸ“ **Format a dataset** for training  
5. ğŸ§‘â€ğŸ« **Train the model with `SFTTrainer`**  
6. ğŸ¤– **Run inference** and test the fine-tuned model  

---

## ğŸ”§ Tools & Frameworks
- **Unsloth** â€“ Fast, lightweight LLM finetuning  
- **LoRA (Low-Rank Adaptation)** â€“ Trainable adapters that save GPU memory & time  
- **PyTorch** â€“ Backend for training  
- **Hugging Face Transformers** â€“ Tokenization & model hub  

---

## âœ¨ Why Unsloth?
Unsloth is an **open-source framework** designed to make LLM fine-tuning:  
âœ… **Faster** (optimized GPU memory usage)  
âœ… **Cheaper** (less compute & VRAM needed)  
âœ… **Accessible** (clean, easy-to-use API)  

Perfect for **students, researchers, and builders** who want to create their own specialized LLMs.  

---

## ğŸ¥ Acknowledgments
This project was inspired by the awesome YouTube channel **[Tech with Tim](https://www.youtube.com/@TechWithTim)** and his video:  
> *â€œEASIEST Way to Fine-Tune a LLM and Use It With Ollamaâ€*  

Big thanks for making LLM finetuning more approachable ğŸ™Œ  

---

## ğŸš€ Getting Started
Clone the repo and open the notebook:

```bash
git clone https://github.com/your-username/llm-finetune-unsloth.git
cd llm-finetune-unsloth
jupyter notebook
